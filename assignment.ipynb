{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "e826bd82-73ec-43d5-d6b5-b8e520e72f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TM10007_ML_g9' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/Doesjka/TM10007_ML_g9.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "iq6XRc6xuYcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import decomposition\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer, accuracy_score, recall_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import neighbors\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import randint"
      ],
      "metadata": {
        "id": "ZGpTRjZRudnE"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "4Eqq7xOFueaX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEUJUe8plrxC"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "-NE_fTbKGe5z"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    data = pd.read_csv('/content/TM10007_ML_g9/worclipo/Lipo_radiomicFeatures.csv', index_col=0)\n",
        "    print(f'The number of features: {len(data.columns)}')\n",
        "    print(f'The number of samples: {len(data.index)}')\n",
        "    data_punten = len(data.index) * len(data.columns)\n",
        "    ls = (data['label'] == 'liposarcoma').sum()\n",
        "    print(f'Of these samples {ls} are liposarcomas. That is {round(ls/len(data.index)*100)} percent.')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data in train and test set\n"
      ],
      "metadata": {
        "id": "LHslq_ZZZbW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(data):\n",
        "    y = data['label']\n",
        "\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    y = lb.fit_transform(y)\n",
        "    y = y.flatten()\n",
        "\n",
        "    X = data.drop('label', axis=1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "5iGiP-NKn1Yp"
      },
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split1(data):\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "P8f1SNKeZaB7"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normaal verdeling en Variantie"
      ],
      "metadata": {
        "id": "L6CG70dYXbs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normaal verdeling\n",
        "Hieronder berekenen we hoe veel van de features normaal verdeeld zijn"
      ],
      "metadata": {
        "id": "BGxgrG6RXe4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normaal_verdeeld(X_train):\n",
        "    aantal_normaal = 0\n",
        "\n",
        "    for column in X_train.columns:\n",
        "        result = stats.shapiro(X_train[column])\n",
        "        normaal = result.pvalue > 0.05\n",
        "        aantal_normaal += normaal\n",
        "\n",
        "    print(aantal_normaal, \" features zijn normaal verdeeld.\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "fU5RKf7NXh0a"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variantie"
      ],
      "metadata": {
        "id": "BKsVP91EXh_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variantie(X_train):\n",
        "    variantie = X_train.var(axis=0)\n",
        "    variantie = variantie.sort_values()\n",
        "    # Haal features met variantie van 0 eruit, want die zeggen dus helemaal niks\n",
        "    zero_variance = variantie.keys()[variantie==0]\n",
        "    X_train = X_train.drop(zero_variance, axis=1)\n",
        "    print(f\"{len(zero_variance)} features have a variance of zero. These features are deleted.\")\n",
        "    print(\"\")\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "i358P9yoXlkj"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing data "
      ],
      "metadata": {
        "id": "gJLsBPlTZwAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Throwing out features\n",
        "All features that exist of at least 50% zeros are deleted from the data. "
      ],
      "metadata": {
        "id": "3MCSfSwrlshM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_features(X_train):\n",
        "    zeros = (X_train == 0).sum()\n",
        "    threshold = 0.5 * len(y_train)\n",
        "    feature_del = zeros[zeros > threshold]\n",
        "\n",
        "    X_train_new = X_train.drop(columns=feature_del.index)\n",
        "    print(f'For {len(X_train.columns)-len(X_train_new.columns)} features the data consisted of more than 50% zeros. These features are deleted.')\n",
        "\n",
        "    more_zeros = (X_train_new == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "    print(f'Of the remaining features, {len(columns_zeros)} features have at least one zero')\n",
        "    print(f'There is a total of {more_zeros.sum()} zeros left in the data')\n",
        "    print(\"\")\n",
        "\n",
        "    return X_train_new"
      ],
      "metadata": {
        "id": "vWcI7Y3g7eDk"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate number of missing values per sample"
      ],
      "metadata": {
        "id": "pkM1c0vkq1YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_samples(X_train):\n",
        "    zeros_r = (X_train == 0).sum(axis=1)\n",
        "    threshold = 0.5 * X_train.size / len(y_train)\n",
        "    sample_del = zeros_r[zeros_r > threshold]\n",
        "    X_train_new = X_train.drop(index=sample_del.index)\n",
        "\n",
        "    print(f'For {len(X_train.index)-len(X_train_new.index)} samples the data consisted of more than 50% zeros. These samples are deleted.')\n",
        "    more_zeros = (X_train_new == 0).sum()\n",
        "    print(f'There is a total of {more_zeros.sum()} zeros left in the data')\n",
        "    print(\"\")\n",
        "    return X_train_new"
      ],
      "metadata": {
        "id": "4gHFgkGMm3Nm"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filling remaining zeros\n",
        "All remaining zeros are replaced by the median of that feature. "
      ],
      "metadata": {
        "id": "KwXkbuDYqhoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_zeros_median(X_train):\n",
        "    more_zeros = (X_train == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "\n",
        "    for column in columns_zeros[:]:\n",
        "        column_median = X_train.loc[X_train[column]!=0, column].median()\n",
        "        X_train[column].replace(0, column_median)\n",
        "        \n",
        "    return X_train\n",
        "\n",
        "def fill_zeros_mean(X_train):\n",
        "    more_zeros = (X_train == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "\n",
        "    for column in columns_zeros[:]:\n",
        "        column_mean = X_train.loc[X_train[column]!=0, column].mean()\n",
        "        X_train[column].replace(0, column_mean)\n",
        "\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "EQudKLJth0sz"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers eruit halen\n"
      ],
      "metadata": {
        "id": "Fui0tjwlW_9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers(X_train):\n",
        "\n",
        "    outliers_total = 0\n",
        "\n",
        "    for column in X_train.columns:\n",
        "        q1 = X_train[column].quantile(0.25)\n",
        "        q3 = X_train[column].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - (1.5 * iqr)\n",
        "        upper_bound = q3 + (1.5 * iqr)\n",
        "\n",
        "        outliers_column = (X_train[column] < lower_bound).sum() + (X_train[column] > upper_bound).sum()\n",
        "        outliers_total += outliers_column\n",
        "\n",
        "        X_train.loc[X_train[column] < lower_bound, column] = lower_bound\n",
        "        X_train.loc[X_train[column] > upper_bound, column] = upper_bound\n",
        "\n",
        "    print(f\"Er zijn {outliers_total} outliers vervangen.\")\n",
        "    print(f\"Dit was {round(outliers_total / (len(data.index) * len(data.columns)) *100)}% van het totale aantal datapunten.\")\n",
        "    print(\"\")\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "ik_99Td7XECw"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "YQtVjjF2dX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaling(X_train):\n",
        "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    return X_train_scaled_df"
      ],
      "metadata": {
        "id": "iXgA9qGcc2eH"
      },
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "ezzBM5civZcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANOVA feature selection for numeric input and categorical output\n",
        "\n",
        "def anova_feature_selection(X_train_scaled_df):\n",
        "    fs = SelectKBest(score_func=f_classif, k='all')\n",
        "\n",
        "    fit = fs.fit(X_train_scaled_df, y_train)\n",
        "    fit = fit.pvalues_\n",
        "    df = pd.DataFrame(fit, columns=['P-value'], index=X_train_scaled_df.columns)\n",
        "    rslt_df = df[df['P-value'] <= 0.05]\n",
        "    transpose = X_train_scaled_df.transpose()\n",
        "    new_df= rslt_df.join(transpose, how='left')\n",
        "    new_df = new_df.transpose()\n",
        "    X_train_ANOVA = new_df.drop('P-value', axis=0)\n",
        "    return X_train_ANOVA"
      ],
      "metadata": {
        "id": "09R4Nb3Mvaxj"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "xOOLNJnlfLbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def principle_components(X_train_ANOVA):\n",
        "    pca = decomposition.PCA()\n",
        "    pca.fit(X_train_ANOVA)\n",
        "    X_pca = pca.transform(X_train_ANOVA)\n",
        "\n",
        "    component = 0\n",
        "    total_ratio = 0\n",
        "    while total_ratio < 0.95:\n",
        "        total_ratio += pca.explained_variance_ratio_[component]\n",
        "        component+=1\n",
        "\n",
        "    component2=0\n",
        "    grens = 0.0005\n",
        "    while pca.explained_variance_ratio_[component2] > grens:\n",
        "        component2+=1\n",
        "\n",
        "    print(\"95 procent van de explained variance ratio geeft\",component, \"componenten. Dit is wat we nu gebruiken\")\n",
        "    print(f\"Het laatste component dat meer dan {grens} bijdraagt aan de explained variance is {component2}\")\n",
        "    print(\"\")\n",
        "    X_pca = X_pca[:,0:component]\n",
        "    return X_pca\n"
      ],
      "metadata": {
        "id": "dU9UMzbtfNTE"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom scoring"
      ],
      "metadata": {
        "id": "wUbcLC9YzdKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_score(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    score = 2 * accuracy * recall / (accuracy + recall)\n",
        "    return score"
      ],
      "metadata": {
        "id": "UXT0yOCLzg6i"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Classification"
      ],
      "metadata": {
        "id": "i0cKvUmo2Uzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA(X_pca,y_train,clsfs):\n",
        "    print(\"Working on the linear classification\")\n",
        "    LDA_classifier = LinearDiscriminantAnalysis()\n",
        "    LDA_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(LDA_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "rjbhcmWJcG0L"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quadratic Discriminant Analysis"
      ],
      "metadata": {
        "id": "5pFcjNm_2ww9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QDA(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the quadratic classification\")\n",
        "    # set parameters\n",
        "    parameters = {'reg_param': np.arange(0, 1, 0.1)}\n",
        "\n",
        "    # Specify the cross validation method to use, we use 10-fold stratified cross-validation\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    # Create QDA object\n",
        "    qda = model_selection.RandomizedSearchCV(QuadraticDiscriminantAnalysis(), parameters, n_iter=11,\n",
        "                                    cv=cv_10fold, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the search\n",
        "    qda.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    qda_df = pd.DataFrame(qda.cv_results_)\n",
        "    # qda_df = qda_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "    # display(qda_df)\n",
        "    print(\"Best parameters for QDA are: \", qda.best_params_)\n",
        "    print(\"Harmonic mean hierbij is:\", qda.best_score_)\n",
        "    print(f\"The mean accuracy score is {qda_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score is {qda_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    QDA_classifier = qda.best_estimator_\n",
        "    QDA_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(QDA_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "K9MLj58j2zkJ"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-NN"
      ],
      "metadata": {
        "id": "xdjw0bA9zOle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KNN(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the KNN classification\")\n",
        "    # Specify the search range, this could be multiple parameters for more complex classifiers\n",
        "    parameters = {'n_neighbors': randint(1, 10),\n",
        "                'weights': ['uniform'],\n",
        "                'p': randint(1, 5),\n",
        "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "                'leaf_size': [2, 4, 10, 30]}\n",
        "\n",
        "    # Specify the cross validation method to use, we use 10-fold stratified cross-validation\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    # Create the grid search method, use area under ROC curve as scoring metric\n",
        "    # Too learn more about metrics see: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "    clf = model_selection.RandomizedSearchCV(neighbors.KNeighborsClassifier(), parameters, cv=cv_10fold, n_iter=500, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the entire search\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    # print(clf_df)\n",
        "\n",
        "    # Extract the best k \n",
        "    # clf_df = clf_df.sort_values(by=['rank_test_score'])\n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "    # optimal_k = clf_df['param_n_neighbors'].iloc[0]\n",
        "    print(\"Best parameters for KNN are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean hierbij is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score is {clf_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score is {clf_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    knn_classifier = clf.best_estimator_\n",
        "    knn_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(knn_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "F2Mc-n3yzRIP"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "u_79CKvDLR_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RF(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the Random Forest classification\")\n",
        "    parameters = {'n_estimators': np.arange(50, 400, 50),\n",
        "                'max_depth': [5, 10, 15],\n",
        "                'min_samples_leaf': [2, 4]}\n",
        "\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "    # print(scoring)\n",
        "    clf = model_selection.RandomizedSearchCV(RandomForestClassifier(), parameters, cv=cv_10fold, n_iter=40, scoring=scoring, refit='harmonic_mean')\n",
        "        \n",
        "    # Fit the classifier\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "\n",
        "    # Extract the best hyperparameters \n",
        "    print(\"Best parameters for RF are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean hierbij is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score is {clf_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score is {clf_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    RF_classifier = clf.best_estimator_\n",
        "    RF_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(RF_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs\n"
      ],
      "metadata": {
        "id": "rpwtmKvoLTjD"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "_O8fWaFjCSl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the SVM classification\")\n",
        "\n",
        "    parameters = {'C': loguniform(0.1, 100),\n",
        "                'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "                'degree': randint(1, 5),\n",
        "                'gamma': loguniform(1e-4, 1e-3),\n",
        "                'class_weight':['balanced', None]}\n",
        "    \n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    clf = model_selection.RandomizedSearchCV(SVC(), parameters, cv=cv_10fold, n_iter=500, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the entire search\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "\n",
        "    # Extract the best hyperparameters \n",
        "    print(\"Best parameters for SVM are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean hierbij is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score is {clf_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score is {clf_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    svm_classifier = clf.best_estimator_\n",
        "    svm_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(svm_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "5MwjArCFCZmH"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning curves"
      ],
      "metadata": {
        "id": "0L14bXT1R6sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Number of training samples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        model_selection.learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "def plot_all_curves(X_train):\n",
        "    fig = plt.figure(figsize=(24,10))\n",
        "    num = 0\n",
        "    for clf in clsfs:\n",
        "        num +=1\n",
        "        ax = fig.add_subplot(2, 3, num)\n",
        "        title = str(type(clf))\n",
        "        plot_learning_curve(clf, title, X_train, y_train, ax, ylim=(0.3, 1.01), cv=10)"
      ],
      "metadata": {
        "id": "1nBrCgircqK1"
      },
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing function"
      ],
      "metadata": {
        "id": "l0RXIEEthMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing(X_train):\n",
        "    # Test hoe veel van de features normaal verdeeld zijn.\n",
        "    normaal_verdeeld(X_train)\n",
        "\n",
        "    # Bepaal de variantie en gooi features eruit die een variantie van nul hebben.\n",
        "    X_train = variantie(X_train)\n",
        "\n",
        "    # Features en samples met te veel nullen eruit gooien\n",
        "    X_train = zeros_features(X_train)\n",
        "    X_train = zeros_samples(X_train)\n",
        "\n",
        "    # Remove outliers\n",
        "    X_train = outliers(X_train)\n",
        "\n",
        "    # Scaling\n",
        "    X_train = scaling(X_train)\n",
        "\n",
        "    # Anova feature selection\n",
        "    X_train = anova_feature_selection(X_train)\n",
        "\n",
        "    # PCA\n",
        "    X_train = principle_components(X_train)\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "f_1XucJjhP7E"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the functions"
      ],
      "metadata": {
        "id": "17WDx8fK0a55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "VskaX3a_qYLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "data = load_data()\n",
        "\n",
        "# Kondig aan bij welke fold we zijn\n",
        "print('Voor nu zijn we alleen nog aan het testen')\n",
        "\n",
        "# Split data\n",
        "X, y = extract_labels(data)\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Pre-processing\n",
        "X_train = pre_processing(X_train)\n",
        "\n",
        "# Initiate classifications\n",
        "clsfs = list()\n",
        "# scoring = make_scorer(custom_score)\n",
        "scoring = {'harmonic_mean': make_scorer(custom_score), 'accuracy': make_scorer(accuracy_score), 'recall': make_scorer(recall_score)}\n",
        "\n",
        "# Classifiers\n",
        "clsfs = LDA(X_train,y_train,clsfs)\n",
        "clsfs = QDA(X_train,y_train,clsfs)\n",
        "clsfs = KNN(X_train,y_train,clsfs)\n",
        "# clsfs = KNN(X_train,y_train,clsfs)\n",
        "# clsfs = KNN(X_train,y_train,clsfs)\n",
        "clsfs = SVM(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "# clsfs = RF(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "\n",
        "# Plot learning curves\n",
        "plot_all_curves(X_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzItnsusqavY",
        "outputId": "863227f7-14a2-4766-ea11-3ff859e74413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of features: 494\n",
            "The number of samples: 115\n",
            "Of these samples 58 are liposarcomas. That is 50 percent.\n",
            "Voor nu zijn we alleen nog aan het testen\n",
            "88  features zijn normaal verdeeld.\n",
            "\n",
            "21 features have a variance of zero. These features are deleted.\n",
            "\n",
            "For 10 features the data consisted of more than 50% zeros. These features are deleted.\n",
            "Of the remaining features, 10 features have at least one zero\n",
            "There is a total of 44 zeros left in the data\n",
            "\n",
            "For 0 samples the data consisted of more than 50% zeros. These samples are deleted.\n",
            "There is a total of 44 zeros left in the data\n",
            "\n",
            "Er zijn 2037 outliers vervangen.\n",
            "Dit was 4% van het totale aantal datapunten.\n",
            "\n",
            "95 procent van de explained variance ratio geeft 17 componenten. Dit is wat we nu gebruiken\n",
            "Het laatste component dat meer dan 0.0005 bijdraagt aan de explained variance is 39\n",
            "\n",
            "Working on the linear classification\n",
            "\n",
            "Working on the quadratic classification\n",
            "Best parameters for QDA are:  {'reg_param': 0.6000000000000001}\n",
            "Harmonic mean hierbij is: 0.7421468573188508\n",
            "The mean accuracy score is 0.6861111111111111\n",
            "The mean recall score is 0.75\n",
            "\n",
            "Working on the KNN classification\n",
            "Best parameters for KNN are:  {'algorithm': 'brute', 'leaf_size': 10, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
            "Harmonic mean hierbij is: 0.7684445896093642\n",
            "The mean accuracy score is 0.7694444444444445\n",
            "The mean recall score is 0.765\n",
            "\n",
            "Working on the SVM classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outer cross validatie"
      ],
      "metadata": {
        "id": "ixaUy9RmkXg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "data = load_data()\n",
        "X, y = extract_labels(data)\n",
        "print(y)\n",
        "\n",
        "# Custom scorer\n",
        "scoring = make_scorer(custom_score)\n",
        "\n",
        "# Define split\n",
        "skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
        "\n",
        "# Initiate classifications\n",
        "clsfs = list()\n",
        "\n",
        "# Loop\n",
        "fig = plt.figure(figsize=(24,10))\n",
        "fold = 0\n",
        "for train, test in skf.split(X,y):\n",
        "    # Kondig aan bij welke fold we zijn\n",
        "    fold += 1\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(f\"Nu bezig met fold \", fold)\n",
        "\n",
        "    # Splits de data\n",
        "    X_train = X.iloc[train,:]\n",
        "    X_test = X.iloc[test,:]\n",
        "    y_train = y[train]\n",
        "    y_test = y[test]\n",
        "\n",
        "    # Pre-processing\n",
        "    X_train = pre_processing(X_train)\n",
        "\n",
        "    # Classifiers\n",
        "    clf = QDA(X_train,y_train,clsfs)[0]\n",
        "    # clsfs = KNN(X_train,y_train,clsfs)\n",
        "    # clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "    # Plotting the learning curve\n",
        "    ax = fig.add_subplot(2, 2, fold)\n",
        "    title = str(f\"Fold {fold}\")\n",
        "    plot_learning_curve(clf, title, X_train, y_train, ax, ylim=(0.3, 1.01), cv=10)\n"
      ],
      "metadata": {
        "id": "-VPlfYLfkaAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}