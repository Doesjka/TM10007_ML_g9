{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doesjka/TM10007_ML_g9/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "a6bc2cea-1342-4b88-e843-17822f1d597d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TM10007_ML_g9' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/Doesjka/TM10007_ML_g9.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "iq6XRc6xuYcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import decomposition\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer, accuracy_score, recall_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import neighbors\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Functions for plotting ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize"
      ],
      "metadata": {
        "id": "ZGpTRjZRudnE"
      },
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "4Eqq7xOFueaX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEUJUe8plrxC"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "-NE_fTbKGe5z"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    data = pd.read_csv('/content/TM10007_ML_g9/worclipo/Lipo_radiomicFeatures.csv', index_col=0)\n",
        "    print(f'The number of features: {len(data.columns)}')\n",
        "    print(f'The number of samples: {len(data.index)}')\n",
        "    data_punten = len(data.index) * len(data.columns)\n",
        "    ls = (data['label'] == 'liposarcoma').sum()\n",
        "    print(f'Of these samples {ls} are liposarcomas. That is {round(ls/len(data.index)*100)} percent.')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data in train and test set\n"
      ],
      "metadata": {
        "id": "LHslq_ZZZbW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(data):\n",
        "    y = data['label']\n",
        "\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    y = lb.fit_transform(y)\n",
        "    y = y.flatten()\n",
        "\n",
        "    X = data.drop('label', axis=1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "5iGiP-NKn1Yp"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split1(data):\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "P8f1SNKeZaB7"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normaal verdeling en Variantie"
      ],
      "metadata": {
        "id": "L6CG70dYXbs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normaal verdeling\n",
        "Hieronder berekenen we hoe veel van de features normaal verdeeld zijn"
      ],
      "metadata": {
        "id": "BGxgrG6RXe4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normaal_verdeeld(X_train):\n",
        "    aantal_normaal = 0\n",
        "\n",
        "    for column in X_train.columns:\n",
        "        result = stats.shapiro(X_train[column])\n",
        "        normaal = result.pvalue > 0.05\n",
        "        aantal_normaal += normaal\n",
        "\n",
        "    print(aantal_normaal, \" features have a normal distribution.\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "fU5RKf7NXh0a"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variantie"
      ],
      "metadata": {
        "id": "BKsVP91EXh_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variantie(X_train):\n",
        "    variantie = X_train.var(axis=0)\n",
        "    variantie = variantie.sort_values()\n",
        "    # Haal features met variantie van 0 eruit, want die zeggen dus helemaal niks\n",
        "    zero_variance = variantie.keys()[variantie==0]\n",
        "    X_train = X_train.drop(zero_variance, axis=1)\n",
        "    print(f\"{len(zero_variance)} features have a variance of zero. These features are deleted.\")\n",
        "    print(\"\")\n",
        "    return X_train, zero_variance"
      ],
      "metadata": {
        "id": "i358P9yoXlkj"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing data "
      ],
      "metadata": {
        "id": "gJLsBPlTZwAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Throwing out features\n",
        "All features that exist of at least 50% zeros are deleted from the data. "
      ],
      "metadata": {
        "id": "3MCSfSwrlshM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_features(X_train):\n",
        "    zeros = (X_train == 0).sum()\n",
        "    threshold = 0.5 * len(y_train)\n",
        "    feature_del = zeros[zeros > threshold]\n",
        "\n",
        "    X_train_new = X_train.drop(columns=feature_del.index)\n",
        "    print(f'For {len(X_train.columns)-len(X_train_new.columns)} features the data consisted of more than 50% zeros. These features are deleted.')\n",
        "\n",
        "    more_zeros = (X_train_new == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "    print(f'Of the remaining features, {len(columns_zeros)} features have at least one zero')\n",
        "    print(f'There is a total of {more_zeros.sum()} zeros left in the data')\n",
        "    print(\"\")\n",
        "\n",
        "    return X_train_new, feature_del"
      ],
      "metadata": {
        "id": "vWcI7Y3g7eDk"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate number of missing values per sample"
      ],
      "metadata": {
        "id": "pkM1c0vkq1YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_samples(X_train):\n",
        "    zeros_r = (X_train == 0).sum(axis=1)\n",
        "    threshold = 0.5 * X_train.size / len(y_train)\n",
        "    sample_del = zeros_r[zeros_r > threshold]\n",
        "    X_train_new = X_train.drop(index=sample_del.index)\n",
        "\n",
        "    print(f'For {len(X_train.index)-len(X_train_new.index)} samples the data consisted of more than 50% zeros. These samples are deleted.')\n",
        "    more_zeros = (X_train_new == 0).sum()\n",
        "    print(f'There is a total of {more_zeros.sum()} zeros left in the data')\n",
        "    print(\"\")\n",
        "    return X_train_new"
      ],
      "metadata": {
        "id": "4gHFgkGMm3Nm"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filling remaining zeros\n",
        "All remaining zeros are replaced by the median of that feature. "
      ],
      "metadata": {
        "id": "KwXkbuDYqhoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_zeros_median(X_train):\n",
        "    more_zeros = (X_train == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "\n",
        "    for column in columns_zeros[:]:\n",
        "        column_median = X_train.loc[X_train[column]!=0, column].median()\n",
        "        X_train[column].replace(0, column_median)\n",
        "        \n",
        "    return X_train\n",
        "\n",
        "def fill_zeros_mean(X_train):\n",
        "    more_zeros = (X_train == 0).sum()\n",
        "    columns_zeros = more_zeros[more_zeros > 0].index\n",
        "\n",
        "    for column in columns_zeros[:]:\n",
        "        column_mean = X_train.loc[X_train[column]!=0, column].mean()\n",
        "        X_train[column].replace(0, column_mean)\n",
        "\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "EQudKLJth0sz"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers eruit halen\n"
      ],
      "metadata": {
        "id": "Fui0tjwlW_9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers(X_train):\n",
        "\n",
        "    outliers_total = 0\n",
        "\n",
        "    for column in X_train.columns:\n",
        "        q1 = X_train[column].quantile(0.25)\n",
        "        q3 = X_train[column].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - (1.5 * iqr)\n",
        "        upper_bound = q3 + (1.5 * iqr)\n",
        "\n",
        "        outliers_column = (X_train[column] < lower_bound).sum() + (X_train[column] > upper_bound).sum()\n",
        "        outliers_total += outliers_column\n",
        "\n",
        "        X_train.loc[X_train[column] < lower_bound, column] = lower_bound\n",
        "        X_train.loc[X_train[column] > upper_bound, column] = upper_bound\n",
        "\n",
        "    print(f\"{outliers_total} outliers were replaced.\")\n",
        "    print(f\"This was {round(outliers_total / (len(data.index) * len(data.columns)) *100)}% of the total amount of datapoints.\")\n",
        "    print(\"\")\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "ik_99Td7XECw"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "YQtVjjF2dX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaling(X_train):\n",
        "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    return X_train_scaled_df, scaler"
      ],
      "metadata": {
        "id": "iXgA9qGcc2eH"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA"
      ],
      "metadata": {
        "id": "ezzBM5civZcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANOVA feature selection for numeric input and categorical output\n",
        "\n",
        "def anova_feature_selection(X_train_scaled_df):\n",
        "    fs = SelectKBest(score_func=f_classif, k='all')\n",
        "\n",
        "    fit = fs.fit(X_train_scaled_df, y_train)\n",
        "    fit = fit.pvalues_\n",
        "    df = pd.DataFrame(fit, columns=['P-value'], index=X_train_scaled_df.columns)\n",
        "    rslt_df = df[df['P-value'] <= 0.1]\n",
        "    transpose = X_train_scaled_df.transpose()\n",
        "    new_df= rslt_df.join(transpose, how='left')\n",
        "    new_df = new_df.transpose()\n",
        "    X_train_ANOVA = new_df.drop('P-value', axis=0)\n",
        "    return X_train_ANOVA, rslt_df"
      ],
      "metadata": {
        "id": "09R4Nb3Mvaxj"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "xOOLNJnlfLbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def principle_components(X_train_ANOVA):\n",
        "    pca = decomposition.PCA()\n",
        "    pca.fit(X_train_ANOVA)\n",
        "    X_pca = pca.transform(X_train_ANOVA)\n",
        "\n",
        "    component = 0\n",
        "    total_ratio = 0\n",
        "    while total_ratio < 0.9:\n",
        "        total_ratio += pca.explained_variance_ratio_[component]\n",
        "        component+=1\n",
        "\n",
        "    component2=0\n",
        "    grens = 0.0001\n",
        "    while pca.explained_variance_ratio_[component2] > grens:\n",
        "        component2+=1\n",
        "\n",
        "    print(\"95 procent of the explained variance ratio gives\",component, \"components. These components we will use from now on.\")\n",
        "    print(f\"The last component that contributes more than {grens} to the explained variance is {component2}.\")\n",
        "    print(\"\")\n",
        "    X_pca = X_pca[:,0:component]\n",
        "    return X_pca, pca, component\n"
      ],
      "metadata": {
        "id": "dU9UMzbtfNTE"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom scoring"
      ],
      "metadata": {
        "id": "wUbcLC9YzdKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_score(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    score = 2 * accuracy * recall / (accuracy + recall)\n",
        "    return score"
      ],
      "metadata": {
        "id": "UXT0yOCLzg6i"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Classification"
      ],
      "metadata": {
        "id": "i0cKvUmo2Uzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA(X_pca,y_train1,clsfs):\n",
        "    print(\"Working on the Linear Discriminant Analysis.\")\n",
        "    LDA_classifier = LinearDiscriminantAnalysis()\n",
        "    LDA_classifier.fit(X_pca, y_train1)\n",
        "    clsfs.append(LDA_classifier)\n",
        "\n",
        "    # Define split\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "    \n",
        "    accuracy_mean = 0\n",
        "    recall_mean = 0\n",
        "    harmonic_mean_mean = 0\n",
        "\n",
        "    for train, test in skf.split(X_pca,y_train1):\n",
        "        # Splits de data\n",
        "        X_train = X.iloc[train,:]\n",
        "        X_val = X.iloc[test,:]\n",
        "        y_train = y[train]\n",
        "        y_true = y[test]\n",
        "\n",
        "        LDA_classifier = LinearDiscriminantAnalysis()\n",
        "        LDA_classifier.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = LDA_classifier.predict(X_val)\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred, average='macro')\n",
        "        harmonic_mean = custom_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "        accuracy_mean += accuracy\n",
        "        recall_mean += recall\n",
        "        harmonic_mean_mean += harmonic_mean\n",
        "\n",
        "    accuracy_mean /= 10\n",
        "    recall_mean /= 10\n",
        "    harmonic_mean_mean /= 10\n",
        "    print(\"Harmonic mean of LDA is:\", harmonic_mean_mean)\n",
        "    print(f\"The mean accuracy score of LDA is {accuracy_mean}\")\n",
        "    print(f\"The mean recall score of LDA is {recall_mean}\")\n",
        "\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "rjbhcmWJcG0L"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quadratic Discriminant Analysis"
      ],
      "metadata": {
        "id": "5pFcjNm_2ww9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QDA(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the Quadratic Discriminant Analysis.\")\n",
        "    # set parameters\n",
        "    parameters = {'reg_param': np.arange(0, 1, 0.1)}\n",
        "\n",
        "    # Specify the cross validation method to use, we use 10-fold stratified cross-validation\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    # Create QDA object\n",
        "    qda = model_selection.RandomizedSearchCV(QuadraticDiscriminantAnalysis(), parameters, n_iter=11,\n",
        "                                    cv=cv_10fold, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the search\n",
        "    qda.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    qda_df = pd.DataFrame(qda.cv_results_)\n",
        "    qda_df = qda_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "    # display(qda_df)\n",
        "    print(\"Best parameters for QDA are: \", qda.best_params_)\n",
        "    print(\"Harmonic mean of QDA is:\", qda.best_score_)\n",
        "    print(f\"The mean accuracy score of QDA is {qda_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score of QDA is {qda_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    QDA_classifier = qda.best_estimator_\n",
        "    QDA_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(QDA_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "K9MLj58j2zkJ"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-NN"
      ],
      "metadata": {
        "id": "xdjw0bA9zOle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KNN(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the KNN classification.\")\n",
        "    # Specify the search range, this could be multiple parameters for more complex classifiers\n",
        "    parameters = {'n_neighbors': randint(1, 10),\n",
        "                'weights': ['uniform'],\n",
        "                'p': randint(1, 5),\n",
        "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "                'leaf_size': [2, 4, 10, 30]}\n",
        "\n",
        "    # Specify the cross validation method to use, we use 10-fold stratified cross-validation\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    # Create the grid search method, use area under ROC curve as scoring metric\n",
        "    # Too learn more about metrics see: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "    clf = model_selection.RandomizedSearchCV(neighbors.KNeighborsClassifier(), parameters, cv=cv_10fold, n_iter=500, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the entire search\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    # print(clf_df)\n",
        "\n",
        "    # Extract the best k \n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "    print(\"Best parameters for KNN are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean of KNN classifier is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score of KNN classifier is {clf_df['mean_test_accuracy'].iloc[0]}.\")\n",
        "    print(f\"The mean recall score of KNN classifier is {clf_df['mean_test_recall'].iloc[0]}.\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    knn_classifier = clf.best_estimator_\n",
        "    knn_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(knn_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "F2Mc-n3yzRIP"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "u_79CKvDLR_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RF(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the Random Forest classification\")\n",
        "    parameters = {'n_estimators': np.arange(50, 400, 50),\n",
        "                'max_depth': [5, 10, 15],\n",
        "                'min_samples_leaf': [2, 4]}\n",
        "\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "    # print(scoring)\n",
        "    clf = model_selection.RandomizedSearchCV(RandomForestClassifier(), parameters, cv=cv_10fold, n_iter=40, scoring=scoring, refit='harmonic_mean')\n",
        "        \n",
        "    # Fit the classifier\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "\n",
        "    # Extract the best hyperparameters \n",
        "    print(\"Best parameters for RF are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean of random forest classifier is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score of random forest classifier is {clf_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score of random forest classifier is {clf_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    RF_classifier = clf.best_estimator_\n",
        "    RF_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(RF_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs\n"
      ],
      "metadata": {
        "id": "rpwtmKvoLTjD"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "_O8fWaFjCSl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM(X_pca, y_train, clsfs):\n",
        "    print(\"Working on the SVM classification.\")\n",
        "\n",
        "    parameters = {'C': loguniform(0.1, 100),\n",
        "                'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "                'degree': randint(1, 5),\n",
        "                'gamma': loguniform(1e-4, 1e-3),\n",
        "                'class_weight':['balanced', None]}\n",
        "    \n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "\n",
        "    clf = model_selection.RandomizedSearchCV(SVC(probability=True), parameters, cv=cv_10fold, n_iter=500, scoring=scoring, refit='harmonic_mean')\n",
        "\n",
        "    # Do the entire search\n",
        "    clf.fit(X_pca, y_train)\n",
        "\n",
        "    # Show the complete results of the cross validation\n",
        "    clf_df = pd.DataFrame(clf.cv_results_)\n",
        "    clf_df = clf_df.sort_values(by=['rank_test_harmonic_mean'])\n",
        "\n",
        "    # Extract the best hyperparameters \n",
        "    print(\"Best parameters for SVM are: \", clf.best_params_)\n",
        "    print(\"Harmonic mean of SVM classifier is:\", clf.best_score_)\n",
        "    print(f\"The mean accuracy score of SVM classifier is {clf_df['mean_test_accuracy'].iloc[0]}\")\n",
        "    print(f\"The mean recall score of SVM classifier is {clf_df['mean_test_recall'].iloc[0]}\")\n",
        "\n",
        "    # Extract the best hyperparameters and fit\n",
        "    svm_classifier = clf.best_estimator_\n",
        "    svm_classifier.fit(X_pca, y_train)\n",
        "    clsfs.append(svm_classifier)\n",
        "    print(\"\")\n",
        "    return clsfs"
      ],
      "metadata": {
        "id": "5MwjArCFCZmH"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning curves"
      ],
      "metadata": {
        "id": "0L14bXT1R6sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Number of training samples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        model_selection.learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "def plot_all_curves(X_train):\n",
        "    fig = plt.figure(figsize=(24,10))\n",
        "    num = 0\n",
        "    for clf in clsfs:\n",
        "        num +=1\n",
        "        ax = fig.add_subplot(2, 3, num)\n",
        "        title = str(type(clf))\n",
        "        plot_learning_curve(clf, title, X_train, y_train, ax, ylim=(0.3, 1.01), cv=10)"
      ],
      "metadata": {
        "id": "1nBrCgircqK1"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC"
      ],
      "metadata": {
        "id": "amaKArW2fKeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_score, y_truth, plt):\n",
        "    '''\n",
        "    Plot an ROC curve.\n",
        "    '''\n",
        "    # Only take scores for class = 1\n",
        "    y_score = y_score[:, 1]\n",
        "    \n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr, tpr, thresholds = roc_curve(y_truth, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Find the index of the threshold value closest to 0.5\n",
        "    idx = np.abs(thresholds - 0.5).argmin()\n",
        "    \n",
        "    # Plot the ROC curve\n",
        "    # plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.scatter(fpr[idx], tpr[idx], marker='x', label='Threshold=0.5')\n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "aiGiq6x0fM_8"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing function"
      ],
      "metadata": {
        "id": "l0RXIEEthMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing_train(X_train):\n",
        "    # Test hoe veel van de features normaal verdeeld zijn.\n",
        "    # normaal_verdeeld(X_train)\n",
        "\n",
        "    # Bepaal de variantie en gooi features eruit die een variantie van nul hebben.\n",
        "    X_train, zero_variance = variantie(X_train)\n",
        "\n",
        "    # Features en samples met te veel nullen eruit gooien\n",
        "    X_train, feature_del = zeros_features(X_train)\n",
        "    X_train = zeros_samples(X_train)\n",
        "\n",
        "    # Remove outliers\n",
        "    X_train = outliers(X_train)\n",
        "\n",
        "    # Scaling\n",
        "    X_train, scaler = scaling(X_train)\n",
        "\n",
        "    # Anova feature selection\n",
        "    X_train, rslt_df = anova_feature_selection(X_train)\n",
        "\n",
        "    # PCA\n",
        "    X_train, pca, component = principle_components(X_train)\n",
        "\n",
        "    return X_train, zero_variance, feature_del, scaler, rslt_df, pca, component"
      ],
      "metadata": {
        "id": "f_1XucJjhP7E"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing_test(X_test, zero_variance, feature_del, scaler, rslt_df, pca, component):\n",
        "    # Test hoe veel van de features normaal verdeeld zijn. Enkel informatief.\n",
        "    normaal_verdeeld(X_test)\n",
        "\n",
        "    # features die eruit gegooid werden op basis van variantie\n",
        "    X_test = X_test.drop(zero_variance, axis=1)\n",
        "\n",
        "    # Features die te veel nullen hadden eruit gooien\n",
        "    X_test = X_test.drop(columns=feature_del.index)\n",
        "\n",
        "    # Remove outliers\n",
        "    # X_test = outliers(X_test)\n",
        "\n",
        "    # Scaling\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "    # Anova feature selection\n",
        "    transpose = X_test.transpose()\n",
        "    new_df= rslt_df.join(transpose, how='left')\n",
        "    new_df = new_df.transpose()\n",
        "    X_test = new_df.drop('P-value', axis=0)\n",
        "\n",
        "    # PCA\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    X_test = X_test_pca[:,0:component]\n",
        "\n",
        "    return X_test"
      ],
      "metadata": {
        "id": "592sPsl2fTvK"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the functions"
      ],
      "metadata": {
        "id": "17WDx8fK0a55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "VskaX3a_qYLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "data = load_data()\n",
        "\n",
        "# Split data\n",
        "X, y = extract_labels(data)\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Pre-processing\n",
        "X_train, zero_variance, feature_del, scaler, rslt_df, pca, component = pre_processing_train(X_train)\n",
        "\n",
        "# Initiate classifications\n",
        "clsfs = list()\n",
        "# scoring = make_scorer(custom_score)\n",
        "scoring = {'harmonic_mean': make_scorer(custom_score), 'accuracy': make_scorer(accuracy_score), 'recall': make_scorer(recall_score)}\n",
        "\n",
        "# Classifiers\n",
        "#clsfs = LDA(X_train,y_train,clsfs)\n",
        "#clsfs = QDA(X_train,y_train,clsfs)\n",
        "#clsfs = KNN(X_train,y_train,clsfs)\n",
        "# clsfs = KNN(X_train,y_train,clsfs)\n",
        "# clsfs = KNN(X_train,y_train,clsfs)\n",
        "clsfs = SVM(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "# clsfs = RF(X_train, y_train, clsfs)\n",
        "# clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "\n",
        "# Plot learning curves\n",
        "plot_all_curves(X_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "jzItnsusqavY",
        "outputId": "1d15cfc8-3466-482b-a7d6-227ed72a4851"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of features: 494\n",
            "The number of samples: 115\n",
            "Of these samples 58 are liposarcomas. That is 50 percent.\n",
            "21 features have a variance of zero. These features are deleted.\n",
            "\n",
            "For 10 features the data consisted of more than 50% zeros. These features are deleted.\n",
            "Of the remaining features, 10 features have at least one zero\n",
            "There is a total of 44 zeros left in the data\n",
            "\n",
            "For 0 samples the data consisted of more than 50% zeros. These samples are deleted.\n",
            "There is a total of 44 zeros left in the data\n",
            "\n",
            "2037 outliers were replaced.\n",
            "This was 4% of the total amount of datapoints.\n",
            "\n",
            "95 procent of the explained variance ratio gives 15 components. These components we will use from now on.\n",
            "The last component that contributes more than 0.0001 to the explained variance is 61.\n",
            "\n",
            "Working on the SVM classification.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-331-a3b601f4ae98>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# clsfs = KNN(X_train,y_train,clsfs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# clsfs = KNN(X_train,y_train,clsfs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mclsfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclsfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# clsfs = SVM(X_train, y_train, clsfs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# clsfs = SVM(X_train, y_train, clsfs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-326-875d1ca101a6>\u001b[0m in \u001b[0;36mSVM\u001b[0;34m(X_pca, y_train, clsfs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Do the entire search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Show the complete results of the cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "gSqeoZnAfjk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pre_processing_test(X_test, zero_variance, feature_del, scaler, rslt_df, pca, component)\n",
        "# print(X_test)\n",
        "clf = clsfs[0]\n",
        "y_pred = clf.predict_proba(X_test)\n",
        "print(y_pred)\n",
        "print(y_test)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "# plot_roc_curve(y_pred, y_test, ax)\n",
        "\n",
        "y_true = y_test\n",
        "harmonic = custom_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(f\"The accuracy is {accuracy}. The recall is {recall}. The harmonic mean of the two is {harmonic}.\")"
      ],
      "metadata": {
        "id": "_WsehP_CflYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outer cross validatie"
      ],
      "metadata": {
        "id": "ixaUy9RmkXg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "data = load_data()\n",
        "X, y = extract_labels(data)\n",
        "\n",
        "# Custom scorer\n",
        "scoring = {'harmonic_mean': make_scorer(custom_score), 'accuracy': make_scorer(accuracy_score), 'recall': make_scorer(recall_score)}\n",
        "\n",
        "# Define split\n",
        "skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
        "\n",
        "# Initiate classifications\n",
        "clsfs = list()\n",
        "\n",
        "# Loop\n",
        "fig = plt.figure(figsize=(24,10))\n",
        "plt.figure()\n",
        "\n",
        "fold = 0\n",
        "for train, test in skf.split(X,y):\n",
        "    # Kondig aan bij welke fold we zijn\n",
        "    fold += 1\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(f\"Working on fold \", fold)\n",
        "\n",
        "    # Splits de data\n",
        "    X_train = X.iloc[train,:]\n",
        "    X_test = X.iloc[test,:]\n",
        "    y_train = y[train]\n",
        "    y_test = y[test]\n",
        "\n",
        "    # Pre-processing\n",
        "    X_train, zero_variance, feature_del, scaler, rslt_df, pca, component = pre_processing_train(X_train)\n",
        "\n",
        "    # Classifier\n",
        "    clsfs = SVM(X_train,y_train,clsfs)\n",
        "    clf = clsfs[fold-1]\n",
        "    # clsfs = KNN(X_train,y_train,clsfs)\n",
        "    # clsfs = SVM(X_train, y_train, clsfs)\n",
        "\n",
        "    # Plotting the learning curve\n",
        "    ax = fig.add_subplot(2, 2, fold)\n",
        "    title = str(f\"Fold {fold}\")\n",
        "    plot_learning_curve(clf, title, X_train, y_train, ax, ylim=(0.3, 1.01), cv=10)\n",
        "\n",
        "    # Start test\n",
        "    X_test = pre_processing_test(X_test, zero_variance, feature_del, scaler, rslt_df, pca, component)\n",
        "    y_score = clf.predict_proba(X_test)\n",
        "\n",
        "    # Plotting the ROC curves\n",
        "    plot_roc_curve(y_score, y_test, plt)\n",
        "\n",
        "    # Scores berekenen\n",
        "    y_true = y_test\n",
        "    y_pred = clf.predict(X_test)\n",
        "    harmonic = custom_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    print(f\"The accuracy is {accuracy}. The recall is {recall}. The harmonic mean of the two is {harmonic}.\")\n",
        "    print(clf)\n",
        "\n",
        "# Finishing the ROC plot\n",
        "lw = 2\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VPlfYLfkaAr",
        "outputId": "dc25a8e0-b44c-4fe8-a35e-7bb562571112"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of features: 494\n",
            "The number of samples: 115\n",
            "Of these samples 58 are liposarcomas. That is 50 percent.\n",
            "-----------------------------------------\n",
            "Working on fold  1\n",
            "20 features have a variance of zero. These features are deleted.\n",
            "\n",
            "For 10 features the data consisted of more than 50% zeros. These features are deleted.\n",
            "Of the remaining features, 12 features have at least one zero\n",
            "There is a total of 42 zeros left in the data\n",
            "\n",
            "For 0 samples the data consisted of more than 50% zeros. These samples are deleted.\n",
            "There is a total of 42 zeros left in the data\n",
            "\n",
            "2151 outliers were replaced.\n",
            "This was 4% of the total amount of datapoints.\n",
            "\n",
            "95 procent of the explained variance ratio gives 17 components. These components we will use from now on.\n",
            "The last component that contributes more than 0.0001 to the explained variance is 64.\n",
            "\n",
            "Working on the SVM classification.\n",
            "Best parameters for SVM are:  {'C': 18.73890969708523, 'class_weight': 'balanced', 'degree': 1, 'gamma': 0.0001104911169988473, 'kernel': 'sigmoid'}\n",
            "Harmonic mean of SVM classifier is: 0.7495028125142985\n",
            "The mean accuracy score of SVM classifier is 0.7541666666666667\n",
            "The mean recall score of SVM classifier is 0.6599999999999999\n",
            "\n",
            "202  features have a normal distribution.\n",
            "\n",
            "The accuracy is 0.7931034482758621. The recall is 0.8666666666666667. The harmonic mean of the two is 0.7917876399834094.\n",
            "SVC(C=18.73890969708523, class_weight='balanced', degree=1,\n",
            "    gamma=0.0001104911169988473, kernel='sigmoid', probability=True)\n",
            "-----------------------------------------\n",
            "Working on fold  2\n",
            "20 features have a variance of zero. These features are deleted.\n",
            "\n",
            "For 10 features the data consisted of more than 50% zeros. These features are deleted.\n",
            "Of the remaining features, 11 features have at least one zero\n",
            "There is a total of 85 zeros left in the data\n",
            "\n",
            "For 0 samples the data consisted of more than 50% zeros. These samples are deleted.\n",
            "There is a total of 85 zeros left in the data\n",
            "\n",
            "2042 outliers were replaced.\n",
            "This was 4% of the total amount of datapoints.\n",
            "\n",
            "95 procent of the explained variance ratio gives 18 components. These components we will use from now on.\n",
            "The last component that contributes more than 0.0001 to the explained variance is 71.\n",
            "\n",
            "Working on the SVM classification.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "dpI6c6WJ6vQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}